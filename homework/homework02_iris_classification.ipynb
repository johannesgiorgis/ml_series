{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 02 Homework: Iris Classification\n",
    "\n",
    "Homework 02 entails us improving the Iris classification models:\n",
    "\n",
    "**Ways to improve on the Iris models**\n",
    "\n",
    "1. Adjust hyperparameters of models\n",
    "2. Add features\n",
    "     - Try Length / Width\n",
    "     - Use Unsupervised model (K-Means)\n",
    "3. Add models to set\n",
    "    - XGBoost\n",
    "    - lightGBM\n",
    "4. Add Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "dataset = pd.read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (150, 5) \n",
      "\n",
      "First records of data:\n",
      "    sepal-length  sepal-width  petal-length  petal-width        class\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa \n",
      "\n",
      "Class Distribution:\n",
      "class\n",
      "Iris-setosa        50\n",
      "Iris-versicolor    50\n",
      "Iris-virginica     50\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# shape\n",
    "print(\"Shape: \", dataset.shape, '\\n')\n",
    "\n",
    "# head\n",
    "print(\"First records of data:\\n\", dataset.head(), '\\n')\n",
    "\n",
    "# class distribution\n",
    "print('Class Distribution:')\n",
    "print(dataset.groupby('class').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical description of data:\n",
      "        sepal-length  sepal-width  petal-length  petal-width\n",
      "count    150.000000   150.000000    150.000000   150.000000\n",
      "mean       5.843333     3.054000      3.758667     1.198667\n",
      "std        0.828066     0.433594      1.764420     0.763161\n",
      "min        4.300000     2.000000      1.000000     0.100000\n",
      "25%        5.100000     2.800000      1.600000     0.300000\n",
      "50%        5.800000     3.000000      4.350000     1.300000\n",
      "75%        6.400000     3.300000      5.100000     1.800000\n",
      "max        7.900000     4.400000      6.900000     2.500000\n"
     ]
    }
   ],
   "source": [
    "print(\"Statistical description of data:\\n\",dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = dataset.values\n",
    "X = array[:,0:4]\n",
    "Y = array[:,4]\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size=validation_size,\n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Shells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LR',\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " ('LDA',\n",
       "  LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                solver='svd', store_covariance=False, tol=0.0001)),\n",
       " ('KNN',\n",
       "  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "             weights='uniform')),\n",
       " ('CART',\n",
       "  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best')),\n",
       " ('RF',\n",
       "  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False)),\n",
       " ('NB', GaussianNB(priors=None, var_smoothing=1e-09)),\n",
       " ('SVM', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)),\n",
       " ('GB', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                n_iter_no_change=None, presort='auto', random_state=None,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "models.append(('GB', GradientBoostingClassifier()))\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot test each model with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.9667 (0.0408)\n",
      "LDA: 0.9750 (0.0382)\n",
      "KNN: 0.9833 (0.0333)\n",
      "CART: 0.9750 (0.0382)\n",
      "RF: 0.9667 (0.0408)\n",
      "NB: 0.9750 (0.0534)\n",
      "SVM: 0.9917 (0.0250)\n",
      "GB: 0.9583 (0.0417)\n"
     ]
    }
   ],
   "source": [
    "# Test options and evaluation metric\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "# evaluate each model in turn\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(\n",
    "        model, X_train, Y_train, cv=kfold, scoring=scoring\n",
    "    )\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = f\"{name}: {cv_results.mean():0.4f} ({cv_results.std():0.4f})\"\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model):\n",
    "    print(model)\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_validation)\n",
    "\n",
    "    print(\"Accuracy Score:\", accuracy_score(Y_validation, predictions))\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(Y_validation, predictions))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy Score: 0.9\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 7  0  0]\n",
      " [ 0 11  1]\n",
      " [ 0  2  9]]\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         7\n",
      "Iris-versicolor       0.85      0.92      0.88        12\n",
      " Iris-virginica       0.90      0.82      0.86        11\n",
      "\n",
      "      micro avg       0.90      0.90      0.90        30\n",
      "      macro avg       0.92      0.91      0.91        30\n",
      "   weighted avg       0.90      0.90      0.90        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_prediction(KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Accuracy Score: 0.8\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 7  0  0]\n",
      " [ 0  7  5]\n",
      " [ 0  1 10]]\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         7\n",
      "Iris-versicolor       0.88      0.58      0.70        12\n",
      " Iris-virginica       0.67      0.91      0.77        11\n",
      "\n",
      "      micro avg       0.80      0.80      0.80        30\n",
      "      macro avg       0.85      0.83      0.82        30\n",
      "   weighted avg       0.83      0.80      0.80        30\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LDA\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)\n",
      "Accuracy Score: 0.9666666666666667\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 7  0  0]\n",
      " [ 0 11  1]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         7\n",
      "Iris-versicolor       1.00      0.92      0.96        12\n",
      " Iris-virginica       0.92      1.00      0.96        11\n",
      "\n",
      "      micro avg       0.97      0.97      0.97        30\n",
      "      macro avg       0.97      0.97      0.97        30\n",
      "   weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "KNN\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy Score: 0.9\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 7  0  0]\n",
      " [ 0 11  1]\n",
      " [ 0  2  9]]\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         7\n",
      "Iris-versicolor       0.85      0.92      0.88        12\n",
      " Iris-virginica       0.90      0.82      0.86        11\n",
      "\n",
      "      micro avg       0.90      0.90      0.90        30\n",
      "      macro avg       0.92      0.91      0.91        30\n",
      "   weighted avg       0.90      0.90      0.90        30\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CART\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Accuracy Score: 0.9\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 7  0  0]\n",
      " [ 0 10  2]\n",
      " [ 0  1 10]]\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         7\n",
      "Iris-versicolor       0.91      0.83      0.87        12\n",
      " Iris-virginica       0.83      0.91      0.87        11\n",
      "\n",
      "      micro avg       0.90      0.90      0.90        30\n",
      "      macro avg       0.91      0.91      0.91        30\n",
      "   weighted avg       0.90      0.90      0.90        30\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RF\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Accuracy Score: 0.8666666666666667\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 7  0  0]\n",
      " [ 0 10  2]\n",
      " [ 0  2  9]]\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         7\n",
      "Iris-versicolor       0.83      0.83      0.83        12\n",
      " Iris-virginica       0.82      0.82      0.82        11\n",
      "\n",
      "      micro avg       0.87      0.87      0.87        30\n",
      "      macro avg       0.88      0.88      0.88        30\n",
      "   weighted avg       0.87      0.87      0.87        30\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NB\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "Accuracy Score: 0.8333333333333334\n",
      "\n",
      "Confusion Matrix:\n",
      " [[7 0 0]\n",
      " [0 9 3]\n",
      " [0 2 9]]\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         7\n",
      "Iris-versicolor       0.82      0.75      0.78        12\n",
      " Iris-virginica       0.75      0.82      0.78        11\n",
      "\n",
      "      micro avg       0.83      0.83      0.83        30\n",
      "      macro avg       0.86      0.86      0.86        30\n",
      "   weighted avg       0.84      0.83      0.83        30\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SVM\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "Accuracy Score: 0.9333333333333333\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 7  0  0]\n",
      " [ 0 10  2]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         7\n",
      "Iris-versicolor       1.00      0.83      0.91        12\n",
      " Iris-virginica       0.85      1.00      0.92        11\n",
      "\n",
      "      micro avg       0.93      0.93      0.93        30\n",
      "      macro avg       0.95      0.94      0.94        30\n",
      "   weighted avg       0.94      0.93      0.93        30\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GB\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "Accuracy Score: 0.9\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 7  0  0]\n",
      " [ 0 10  2]\n",
      " [ 0  1 10]]\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         7\n",
      "Iris-versicolor       0.91      0.83      0.87        12\n",
      " Iris-virginica       0.83      0.91      0.87        11\n",
      "\n",
      "      micro avg       0.90      0.90      0.90        30\n",
      "      macro avg       0.91      0.91      0.91        30\n",
      "   weighted avg       0.90      0.90      0.90        30\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model in models:\n",
    "    print(name)\n",
    "    make_prediction(model)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use K Nearest Neighbor with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johannes/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': np.arange(1, 20)\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(knn, param_grid, cv=10)\n",
    "\n",
    "knn_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9916666666666667 | Best Parameter K:  {'n_neighbors': 13}\n"
     ]
    }
   ],
   "source": [
    "print('Best Score:', knn_cv.best_score_,'| Best Parameter K: ', knn_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a prediction using the suggest model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=13, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy Score: 0.9\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 7  0  0]\n",
      " [ 0 10  2]\n",
      " [ 0  1 10]]\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         7\n",
      "Iris-versicolor       0.91      0.83      0.87        12\n",
      " Iris-virginica       0.83      0.91      0.87        11\n",
      "\n",
      "      micro avg       0.90      0.90      0.90        30\n",
      "      macro avg       0.91      0.91      0.91        30\n",
      "   weighted avg       0.90      0.90      0.90        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_prediction(KNeighborsClassifier(n_neighbors=13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "# map the iris class to numerical values\n",
    "class_map = {\n",
    "    'Iris-setosa': 0,\n",
    "    'Iris-versicolor': 1,\n",
    "    'Iris-virginica': 2\n",
    "}\n",
    "Y_new = [class_map[iris_class] for iris_class in Y]\n",
    "\n",
    "# split data into train and validation sets\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(\n",
    "    X,\n",
    "    Y_new,\n",
    "    test_size=validation_size,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=Y_train)\n",
    "d_validate = xgb.DMatrix(X_validation, label=Y_validation)\n",
    "\n",
    "# set xgboost params\n",
    "param = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.3,\n",
    "    'silent': 1,\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3\n",
    "}\n",
    "num_round = 20\n",
    "\n",
    "# train model\n",
    "bst = xgb.train(param, d_train, num_round)\n",
    "\n",
    "# predict\n",
    "predictions = bst.predict(d_validate)\n",
    "\n",
    "# pick best prediction results\n",
    "best_predictions = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "print(accuracy_score(Y_validation, best_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    "Using XGBoost didn't improve our prediction result. \n",
    "\n",
    "One way to further improve this is by using GridSearch to perform an exhaustive search over specified parameter values for an estimator. This is shown below, which didn't end up working so well. Kept there for future learning/reference purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 324 out of 324 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "# map the iris class to numerical values\n",
    "class_map = {\n",
    "    'Iris-setosa': 0,\n",
    "    'Iris-versicolor': 1,\n",
    "    'Iris-virginica': 2\n",
    "}\n",
    "Y_new = [class_map[iris_class] for iris_class in Y]\n",
    "\n",
    "# split data into train and validation sets\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(\n",
    "    X,\n",
    "    Y_new,\n",
    "    test_size=validation_size,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=Y_train)\n",
    "d_validate = xgb.DMatrix(X_validation, label=Y_validation)\n",
    "\n",
    "# instantiate model\n",
    "\n",
    "# set static model parameters\n",
    "param = {\n",
    "    'silent': 1, # verbosity didn't work although it is recommended in the docs as silent is deprecated\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3\n",
    "}\n",
    "num_round = 20\n",
    "\n",
    "model = xgb.XGBClassifier(param)\n",
    "\n",
    "# set param_grid\n",
    "param_grid = {\n",
    "    'max_depth': np.arange(2, 13),\n",
    "    'eta': [0.1,0.2,0.3,0.4]\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'num_boost_round': [100, 250, 500],\n",
    "    'eta': [0.05, 0.1, 0.3],\n",
    "    'max_depth': [6, 9, 12],\n",
    "    'subsample': [0.9, 1.0],\n",
    "    'colsample_bytree': [0.9, 1.0],\n",
    "}\n",
    "\n",
    "xgb_cv = GridSearchCV(model, param_grid=param_grid, cv=3, verbose=1).fit(X_train, Y_train)\n",
    "\n",
    "# train model\n",
    "#bst = xgb.train(param, d_train, num_round)\n",
    "\n",
    "# predict\n",
    "#predictions = bst.predict(d_validate)\n",
    "\n",
    "# pick best prediction results\n",
    "#best_predictions = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "#print(accuracy_score(Y_validation, best_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth={'silent': 1, 'objective': 'multi:softprob', 'num_class': 3},\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'num_boost_round': [100, 250, 500], 'eta': [0.05, 0.1, 0.3], 'max_depth': [6, 9, 12], 'subsample': [0.9, 1.0], 'colsample_bytree': [0.9, 1.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.9, eta=0.05, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=6, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None, num_boost_round=100,\n",
       "       objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=0.9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameter setting\n",
    "xgb_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23333333333333334\n"
     ]
    }
   ],
   "source": [
    "best_model = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.9, eta=0.05, gamma=0, learning_rate=0.1,\n",
    "       max_delta_step=0, max_depth=6, min_child_weight=1, missing=None,\n",
    "       n_estimators=100, n_jobs=1, nthread=None, num_boost_round=100,\n",
    "       objective='multi:softprob', random_state=0, reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
    "       subsample=0.9)\n",
    "\n",
    "\n",
    "# train model\n",
    "best_model.fit(X_train, Y_train)\n",
    "\n",
    "# predict\n",
    "predictions = best_model.predict(X_validation)\n",
    "\n",
    "# pick best prediction results\n",
    "best_predictions = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "print(accuracy_score(Y_validation, best_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- [Simple XGBoost Tutorial](https://www.kdnuggets.com/2017/03/simple-xgboost-tutorial-iris-dataset.html)\n",
    "- [GridSearch XGBoost with Scikit-Learn](https://www.kaggle.com/tanitter/grid-search-xgboost-with-scikit-learn)\n",
    "- [XGBoost Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
    "- [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "- [Scikit-Learn Classifiers on Iris Dataset](https://www.kaggle.com/chungyehwang/scikit-learn-classifiers-on-iris-dataset)\n",
    "- [Iris Dataset EDA](https://www.kaggle.com/lalitharajesh/iris-dataset-exploratory-data-analysis)\n",
    "- [XGBoost with Scikit-Learn](https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn)\n",
    "- [Using clustering for feature engineering on the Iris Dataset](https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn)\n",
    "- [Iris Dataset Kmeans](https://nbviewer.jupyter.org/github/arunabh15091989/Iris-Kmeans/blob/master/KMeans_Iris.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
